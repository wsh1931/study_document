# 感知机

​	本章将介绍感知机（perceptron）这一算法。感知机是由美国学者Frank Rosenblatt 在 1957 年提出来的。为何我们现在还要学习这一很久以前就有 的算法呢？因为感知机也是作为神经网络（深度学习）的起源的算法。因此， 学习感知机的构造也就是学习通向神经网络和深度学习的一种重要思想。

## 感知机是什么

​	感知机接收多个输入信号，输出一个信号。这里所说的“信号”可以想 象成电流或河流那样具备“流动性”的东西。像电流流过导线， 向前方输送 电子一样，感知机的信号也会形成流， 向前方输送信息。但是，和实际的电 流不同的是，感知机的信号只有“流 / 不流”（1/0）两种取值。在本书中，0 对应“不传递信号”，1 对应“传递信号”。

​	图 2-1 是一个接收两个输入信号的感知机的例子。x1 、x2 是输入信号， y 是输出信号，w1 、w2 是权重（w 是 weight 的首字母)。图中的○称为“神 经元”或者“节点”。输入信号被送往神经元时，会被分别乘以固定的权重（w1x1 、w2x2）。神经元会计算传送过来的信号的总和，只有当这个总和超过 了某个界限值时，才会输出 1。这也称为“神经元被激活”。这里将这个界 限值称为阈值，用符号θ表示。

![image-20250820100828144](images/image-20250820100828144.png)

​									图 2 - 1  有两个输入的感知机

感知机的运行原理只有这些！把上述内容用数学式来表示，就是式（2.1）。

![image-20250820100905945](images/image-20250820100905945.png)

​	感知机的多个输入信号都有各自固有的权重，这些权重发挥着控制各个 信号的重要性的作用。也就是说，权重越大，对应该权重的信号的重要性就 越高。

​	权重相当于电流里所说的电阻。电阻是决定电流流动难度的参数， 电阻越低，通过的电流就越大。而感知机的权重则是值越大，通过 的信号就越大。不管是电阻还是权重，在控制信号流动难度（或者流 动容易度）这一点上的作用都是一样的。

## 简单逻辑电路

### 与门

​	现在让我们考虑用感知机来解决简单的问题。这里首先以逻辑电路为题 材来思考一下与门（AND gate）。与门是有两个输入和一个输出的门电路。图2-2 这种输入信号和输出信号的对应表称为“真值表”。如图 2-2 所示，与门仅在 两个输入均为 1 时输出 1，其他时候则输出0。

![image-20250820101456984](images/image-20250820101456984.png)

​											图 2 - 2   与门的真值表

​	下面考虑用感知机来表示这个与门。需要做的就是确定能满足图 2 - 2 的 真值表的 w1 、w2 、θ的值。那么，设定什么样的值才能制作出满足图 2 - 2 的 条件的感知机呢？

​	实际上，满足图 2-2 的条件的参数的选择方法有无数多个。比如，当 (w1 , w2, θ) = (0.5, 0.5, 0.7) 时，可以满足图 2-2 的条件。此外，当 (w1 , w2, θ) 为(0.5, 0.5, 0.8) 或者(1.0, 1.0, 1.0) 时， 同样也满足与门的条件。设定这样的 参数后，仅当 x1 和 x2 同时为 1 时，信号的加权总和才会超过给定的阈值θ。

### 与非门

​	接着，我们再来考虑一下与非门（NAND gate）。NAND 是Not AND 的意思，与非门就是颠倒了与门的输出。用真值表表示的话，如图 2-3 所示， 仅当 x1 和 x2 同时为 1 时输出 0，其他时候则输出 1。那么与非门的参数又可 以是什么样的组合呢？

![image-20250820105031598](images/image-20250820105031598.png)

​											图 2-3   与非门的真值表

​	要表示与非门，可以用 (w1 , w2, θ) = (-0.5, -0.5, -0.7) 这样的组合（其 他的组合也是无限存在的）。实际上，只要把实现与门的参数值的符号取反， 就可以实现与非门。

​	这里决定感知机参数的并不是计算机，而是我们人。我们看着真值 表这种“训练数据”，人工考虑（想到）了参数的值。而机器学习的课 题就是将这个决定参数值的工作交由计算机自动进行。学习是确定 合适的参数的过程，而人要做的是思考感知机的构造（模型)，并把 训练数据交给计算机。

​	如上所示，我们已经知道使用感知机可以表示与门、与非门、或门的逻 辑电路。这里重要的一点是：与门、与非门、或门的感知机构造是一样的。 实际上，3 个门电路只有参数的值（权重和阈值）不同。也就是说，相同构造 的感知机，只需通过适当地调整参数的值，就可以像“变色龙演员”表演不  同的角色一样，变身为与门、与非门、或门。

## 感知机的实现

### 简单的实现

现在，我们用Python 来实现刚才的逻辑电路。这里，先定义一个接收 参数x1 和x2 的AND 函数

```py
def  AND(x1,  x2):
    w1,  w2,  theta  =  0.5,  0.5,  0.7
    tmp  =  x1*w1  +  x2*w2
    if  tmp  <=  theta:
        return  0
    elif  tmp  >  theta:
        return  1


#  输出 0
print(AND(0,  0))
#  输出 0
print(AND(1,  0))
#  输出 0
print(AND(0,  1))
#  输出 1
print(AND(1,  1))
```

### 导入权重和偏置

​	刚才的与门的实现比较直接、容易理解，但是考虑到以后的事情，我们 将其修改为另外一种实现形式。在此之前，首先把式（2.1）的θ换成-b，于 是就可以用式（2.2）来表示感知机的行为。

![image-20250820110514322](images/image-20250820110514322.png)

​	式（2.1）和式（2.2）虽然有一个符号不同，但表达的内容是完全相同的。 此处，b 称为偏置，`w1` 和 `w2` 称为权重。如式（2.2）所示，感知机会计算输入 信号和权重的乘积，然后加上偏置，如果这个值大于`0` 则输出1，否则输出0。 下面，我们使用NumPy，按式（2.2）的方式实现感知机。在这个过程中，我 们用Python 的解释器逐一确认结果。

```py
>>>  import  numpy  as  np
>>>  x  =  np.array([0,  1])  #  输入
>>>  w  =  np.array([0.5,  0.5])  #  权重
>>>  b  =  -0.7  #  偏置
>>>  w*x
array([  0.  ,    0.5])
>>>  np.sum(w*x)
0.5
>>>  np.sum(w*x)  +  b
-0.19999999999999996    #  大约为 -0.2（由浮点小数造成的运算误差）
```

​	如上例所示，在NumPy 数组的乘法运算中，当两个数组的元素个数相同时， 各个元素分别相乘，因此`w`*`x` 的结果就是它们的各个元素分别相乘（ [0, 1]  *  [0.5, 0.5] =>  [0, 0.5]）。之后，np.sum(w*x) 再计算相乘后的各个元素的总和。 最后再把偏置加到这个加权总和上，就完成了式（2.2）的计算。

### 使用权重和偏置的实现

使用权重和偏置，可以像下面这样实现与门。

```py
def  AND(x1,  x2):
    x  =  np .array([x1,  x2])
    w  =  np.array([0.5,  0.5])
    b  =  -0.7
    tmp  =  np.sum(w*x)  +  b
    if  tmp  <=  0:
    	return  0
    else:
   	 return  1
```

​	这里把-θ命名为偏置 b，但是请注意，偏置和权重 w1 、w2 的作用是不 一样的。具体地说，w1 和 w2 是控制输入信号的重要性的参数，而偏置是调 整神经元被激活的容易程度（输出信号为 1 的程度）的参数。比如，若 b 为-0.1，则只要输入信号的加权总和超过 0.1，神经元就会被激活。但是如果 b  为-20.0，则输入信号的加权总和必须超过 20.0，神经元才会被激活。像这样， 偏置的值决定了神经元被激活的容易程度。另外，这里我们将 w1 和 w2 称为权重， 将 b 称为偏置，但是根据上下文，有时也会将 b、w1、w2 这些参数统称为权重。

接着，我们继续实现与非门和或门。

```py
def  NAND(x1,  x2):
    x  =  np .array([x1,  x2])
    w  =  np.array([-0.5,   -0.5])  #  仅权重和偏置与 AND 不同！ b  =  0.7
    tmp  =  np.sum(w*x)  +  b
    if  tmp  <=  0:
    	return  0
    else:
    	return  1
```

```py
def  OR(x1,  x2):
    x  =  np .array([x1,  x2])
    w  =  np.array([0.5,  0.5])  #  仅权重和偏置与 AND 不同！
    b  =  -0.2
    tmp  =  np.sum(w*x)  +  b
    if  tmp  <=  0:
    	return  0
    else:
    	return  1
```

​	我们在 2.2 节介绍过，与门、与非门、或门是具有相同构造的感知机， 区别只在于权重参数的值。因此，在与非门和或门的实现中，仅设置权重和  偏置的值这一点和与门的实现不同。

## 感知机的局限性

到这里我们已经知道，使用感知机可以实现与门、与非门、或门三种逻 辑电路。现在我们来考虑一下异或门（XOR gate）。

### 异或门

​	异或门也被称为逻辑异或电路。如图 2-5 所示，仅当 x1 或 x2 中的一方为 1 时，才会输出 1（“异或”是拒绝其他的意思）。那么，要用感知机实现这个 异或门的话，应该设定什么样的权重参数呢？

![image-20250820112708714](images/image-20250820112708714.png)

​											**图 2 - 5  异或门的真值表**

​	实际上，用前面介绍的感知机是无法实现这个异或门的。为什么用感知 机可以实现与门、或门，却无法实现异或门呢？下面我们尝试通过画图来思 考其中的原因。

​	首先，我们试着将或门的动作形象化。或门的情况下，当权重参数 (b, w1 , w2) = (-0.5, 1.0, 1.0) 时，可满足图 2-4 的真值表条件。此时，感知机 可用下面的式（2.3）表示。

![image-20250820112810019](images/image-20250820112810019.png)

​	式（2.3）表示的感知机会生成由直线-0.5 + x1  + x2  = 0 分割开的两个空 间。其中一个空间输出 1，另一个空间输出 0，如图 2-6 所示。

![image-20250820112840185](images/image-20250820112840185.png)

​	或门在 (x1 , x2) = (0, 0) 时输出 0，在 (x1 , x2) 为 (0, 1) 、(1, 0) 、(1, 1) 时输 出 1。图 2-6 中，○表示 0， △表示 1。如果想制作或门，需要用直线将图2 - 6中的○和△分开。实际上，刚才的那条直线就将这 4 个点正确地分开了。

那么，换成异或门的话会如何呢？能否像或门那样，用一条直线作出分 割图 2-7 中的○和△的空间呢？

![image-20250820112909535](images/image-20250820112909535.png)

​	想要用一条直线将图2-7 中的○和△分开，无论如何都做不到。事实上， 用一条直线是无法将○和△分开的。

### 线性和非线性

​	图 2-7 中的○和△无法用一条直线分开，但是如果将“直线”这个限制条  件去掉，就可以实现了。比如，我们可以像图2-8 那样，作出分开○和△的空间。

​	感知机的局限性就在于它只能表示由一条直线分割的空间。图2-8这样弯 曲的曲线无法用感知机表示。另外，由图2-8这样的曲线分割而成的空间称为 非线性空间，由直线分割而成的空间称为线性空间。线性、非线性这两个术 语在机器学习领域很常见，可以将其想象成图2-6和图2-8所示的直线和曲线。

![image-20250820113230788](images/image-20250820113230788.png)

## 多层感知机

​	感知机不能表示异或门让人深感遗憾，但也无需悲观。实际上，感知机  的绝妙之处在于它可以“叠加层”（通过叠加层来表示异或门是本节的要点）。 这里，我们暂且不考虑叠加层具体是指什么，先从其他视角来思考一下异或门的问题。

### 已有门电路的组合

​	异或门的制作方法有很多，其中之一就是组合我们前面做好的与门、与 非门、或门进行配置。这里，与门、与非门、或门用图 2-9 中的符号表示。另外， 图 2-9 中与非门前端的○表示反转输出的意思。

​	那么，请思考一下，要实现异或门的话，需要如何配置与门、与非门和 或门呢？这里给大家一个提示，用与门、与非门、或门代替图2-10 中的各个 “? ”,就可以实现异或门。

![image-20250820113945912](images/image-20250820113945912.png)

​	异或门可以通过图 2-11所示的配置来实现。这里，x1 和 x2 表示输入信号， y表示输出信号。x1 和 x2 是与非门和或门的输入，而与非门和或门的输出则 是与门的输入。

![image-20250820114017778](images/image-20250820114017778.png)

​	现在，我们来确认一下图2-11的配置是否真正实现了异或门。这里，把 s1 作为与非门的输出，把 s2 作为或门的输出，填入真值表中。结果如图 2-12 所示，观察 x1 、x2 、y，可以发现确实符合异或门的输出。

![image-20250820114036695](images/image-20250820114036695.png)

### 异或门的实现

​	下面我们试着用Python 来实现图 2-11 所示的异或门。使用之前定义的 AND 函数、NAND 函数、OR 函数，可以像下面这样（轻松地）实现。

```py
def  XOR(x1,  x2):
    s1  =  NAND(x1,  x2)
    s2  =  OR(x1,  x2)
    y  =  AND(s1,  s2)
    return  y
XOR(0,  0)  #  输出 0
XOR(1,  0)  #  输出 1
XOR(0,  1)  #  输出 1
XOR(1,  1)  #  输出 0
```

这样，异或门的实现就完成了。下面我们试着用感知机的表示方法（明 确地显示神经元）来表示这个异或门，结果如图 2-13 所示。

​	如图 2-13 所示，异或门是一种多层结构的神经网络。这里，将最左边的 一列称为第 0 层，中间的一列称为第 1 层，最右边的一列称为第 2 层。

​	图 2-13 所示的感知机与前面介绍的与门、或门的感知机（图 2-1）形状不 同。实际上，与门、或门是单层感知机，而异或门是 2 层感知机。叠加了多 层的感知机也称为多层感知机（multi-layered perceptron）。

![image-20250820114514413](images/image-20250820114514413.png)

​									图 2-13   用感知机表示异或门

​	图 2-13 中的感知机总共由 3 层构成，但是因为拥有权重的层实质 上只有 2 层（第 0 层和第 1 层之间，第 1 层和第 2 层之间)，所以称 为“2 层感知机”。不过，有的文献认为图 2-13 的感知机是由 3 层 构成的，因而将其称为“3 层感知机”。

​	在图 2-13 所示的 2 层感知机中，先在第 0 层和第 1 层的神经元之间进行 信号的传送和接收，然后在第 1 层和第 2 层之间进行信号的传送和接收，具 体如下所示。

1. 第0 层的两个神经元接收输入信号，并将信号发送至第1 层的神经元。
2. 第 1 层的神经元将信号发送至第2 层的神经元，第2 层的神经元输出y。

​	这种 2 层感知机的运行过程可以比作流水线的组装作业。第 1 段（第 1 层） 的工人对传送过来的零件进行加工，完成后再传送给第 2 段（第 2 层）的工人。 第 2 层的工人对第 1 层的工人传过来的零件进行加工，完成这个零件后出货  （输出）。

​	像这样，在异或门的感知机中，工人之间不断进行零件的传送。通过这 样的结构（2 层结构），感知机得以实现异或门。这可以解释为“单层感知机 无法表示的东西，通过增加一层就可以解决”。也就是说，通过叠加层（加深 层），感知机能进行更加灵活的表示。

## 从与非门到计算机

​	多层感知机可以实现比之前见到的电路更复杂的电路。比如，进行加法 运算的加法器也可以用感知机实现。此外，将二进制转换为十进制的编码器、 满足某些条件就输出1的电路（用于等价检验的电路）等也可以用感知机表示。 实际上，使用感知机甚至可以表示计算机！

​	计算机是处理信息的机器。向计算机中输入一些信息后，它会按照某种   既定的方法进行处理，然后输出结果。所谓“按照某种既定的方法进行处理” 是指，计算机和感知机一样，也有输入和输出，会按照某个既定的规则进行   计算。

​	人们一般会认为计算机内部进行的处理非常复杂，而令人惊讶的是，实  际上只需要通过与非门的组合，就能再现计算机进行的处理。这一令人吃惊 的事实说明了什么呢？说明使用感知机也可以表示计算机。前面也介绍了， 与非门可以使用感知机实现。也就是说，如果通过组合与非门可以实现计算 机的话，那么通过组合感知机也可以表示计算机（感知机的组合可以通过叠  加了多层的单层感知机来表示)。

​	综上，多层感知机能够进行复杂的表示，甚至可以构建计算机。那么， 什么构造的感知机才能表示计算机呢？层级多深才可以构建计算机呢？

​	理论上可以说 2层感知机就能构建计算机。这是因为，已有研究证明， 2层感知机（严格地说是激活函数使用了非线性的sigmoid函数的感知机，具 体请参照下一章）可以表示任意函数。但是，使用 2层感知机的构造，通过设定合适的权重来构建计算机是一件非常累人的事情。实际上，在用与非门 等低层的元件构建计算机的情况下，分阶段地制作所需的零件（模块）会比 较自然，即先实现与门和或门，然后实现半加器和全加器，接着实现算数逻 辑单元（ALU），然后实现CPU。因此，通过感知机表示计算机时，使用叠 加了多层的构造来实现是比较自然的流程。

​	本书中不会实际来实现计算机，但是希望读者能够记住，感知机通过叠 加层能够进行非线性的表示，理论上还可以表示计算机进行的处理。

## 小结

​	本章我们学习了感知机。感知机是一种非常简单的算法，大家应该很快 就能理解它的构造。感知机是下一章要学习的神经网络的基础， 因此本章的 内容非常重要。

 感知机是具有输入和输出的算法。给定一个输入后，将输出一个既 定的值。

* 感知机将权重和偏置设定为参数。

* 使用感知机可以表示与门和或门等逻辑电路。

* 异或门无法通过单层感知机来表示。

* 使用2 层感知机可以表示异或门。

* 单层感知机只能表示线性空间，而多层感知机可以表示非线性空间。

* 多层感知机（在理论上）可以表示计算机。

# 神经网络

​	上一章我们学习了感知机。关于感知机，既有好消息，也有坏消息。好 消息是，即便对于复杂的函数，感知机也隐含着能够表示它的可能性。上一 章已经介绍过，即便是计算机进行的复杂处理，感知机（理论上）也可以将 其表示出来。坏消息是，设定权重的工作，即确定合适的、能符合预期的输 入与输出的权重，现在还是由人工进行的。上一章中，我们结合与门、或门 的真值表人工决定了合适的权重。

​	神经网络的出现就是为了解决刚才的坏消息。具体地讲，神经网络的一 个重要性质是它可以自动地从数据中学习到合适的权重参数。本章中，我们 会先介绍神经网络的概要，然后重点关注神经网络进行识别时的处理。在下一章中，我们将了解如何从数据中学习权重参数。

## 从感知机到神经网络

神经网络和上一章介绍的感知机有很多共同点。这里，我们主要以两者 的差异为中心，来介绍神经网络的结构。

### 神经网络的例子

​	用图来表示神经网络的话，如图 3-1 所示。我们把最左边的一列称为 输入层，最右边的一列称为输出层， 中间的一列称为中间层。中间层有时也称为隐藏层。“隐藏”一词的意思是， 隐藏层的神经元（和输入层、输出 层不同）肉眼看不见。另外，本书中把输入层到输出层依次称为第 0 层、第 1 层、第 2 层（层号之所以从 0 开始，是为了方便后面基于 Python 进行实现）。 图 3-1 中，第 0 层对应输入层，第 1 层对应中间层，第 2 层对应输出层。

![image-20250820140940232](images/image-20250820140940232.png)

​	图 3-1 中的网络一共由 3 层神经元构成，但实质上只有 2 层神经 元有权重， 因此将其称为“2 层网络”。请注意，有的书也会根据 构成网络的层数，把图 3-1 的网络称为“3 层网络”。本书将根据 实质上拥有权重的层数（输入层、隐藏层、输出层的总数减去 1 后的数量)来表示网络的名称。

​	只看图 3-1 的话，神经网络的形状类似上一章的感知机。实际上，就神 经元的连接方式而言，与上一章的感知机并没有任何差异。那么，神经网络 中信号是如何传递的呢？

### 复习感知机

在观察神经网络中信号的传递方法之前，我们先复习一下感知机。现在来思考一下图 3-2 中的网络结构。

![image-20250820141407418](images/image-20250820141407418.png)

​											图 3 - 2   复习感知机

图 3-2 中的感知机接收 x1 和 x2 两个输入信号，输出 y。如果用数学式来表示图 3-2 中的感知机，则如式（3.1）所示。

![image-20250820141445740](images/image-20250820141445740.png)

​	b 是被称为偏置的参数，用于控制神经元被激活的容易程度；而 w1 和 w2 是表示各个信号的权重的参数，用于控制各个信号的重要性。

​	顺便提一下，在图 3-2 的网络中，偏置 b 并没有被画出来。如果要明确 地表示出 b，可以像图 3-3 那样做。图 3-3 中添加了权重为 b 的输入信号 1。这 个感知机将 x1、x2、1 三个信号作为神经元的输入，将其和各自的权重相乘后， 传送至下一个神经元。在下一个神经元中，计算这些加权信号的总和。如果  这个总和超过 0，则输出 1，否则输出 0。另外，由于偏置的输入信号一直是 1， 所以为了区别于其他神经元，我们在图中把这个神经元整个涂成灰色。

​	现在将式（3.1）改写成更加简洁的形式。为了简化式（3.1），我们用一个 函数来表示这种分情况的动作（超过 0 则输出 1，否则输出 0）。引入新函数 h(x)，将式（3.1）改写成下面的式（3.2）和式（3.3）。

![image-20250820141517108](images/image-20250820141517108.png)

![image-20250820141536430](images/image-20250820141536430.png)

​	式（3.2）中，输入信号的总和会被函数 h(x) 转换，转换后的值就是输出y。 然后，式（3.3）所表示的函数 h(x)，在输入超过 0 时返回 1，否则返回 0。因此， 式（3.1）和式（3.2）、式（3.3）做的是相同的事情。

### 激活函数登场

​	刚才登场的 h（x）函数会将输入信号的总和转换为输出信号，这种函数 一般称为激活函数（activation function）。如“激活”一词所示，激活函数的 作用在于决定如何来激活输入信号的总和。

​	现在来进一步改写式（3.2）。式（3.2）分两个阶段进行处理，先计算输入 信号的加权总和，然后用激活函数转换这一总和。因此，如果将式（3.2）写 得详细一点，则可以分成下面两个式子。

![image-20250820142119177](images/image-20250820142119177.png)

首先，式（3.4）计算加权输入信号和偏置的总和，记为a。然后，式（3.5） 用 h() 函数将 a 转换为输出y。

之前的神经元都是用一个○表示的，如果要在图中明确表示出式（3.4） 和式（3.5），则可以像图 3-4 这样做。

![image-20250820142149466](images/image-20250820142149466.png)

​	如图 3-4 所示，表示神经元的○中明确显示了激活函数的计算过程，即 信号的加权总和为节点 a，然后节点 a 被激活函数 h() 转换成节点 y。本书中，“神 经元”和“节点”两个术语的含义相同。这里，我们称 a 和 y 为“节点”，其实 它和之前所说的“神经元”含义相同。

​	通常如图 3-5 的左图所示，神经元用一个○表示。本书中，在可以明确 神经网络的动作的情况下，将在图中明确显示激活函数的计算过程，如图 3-5 的右图所示。

![image-20250820142204907](images/image-20250820142204907.png)

下面，我们将仔细介绍激活函数。激活函数是连接感知机和神经网络的 桥梁。

​	本书在使用“感知机”一词时，没有严格统一它所指的算法。一 般而言，“朴素感知机”是指单层网络，指的是激活函数使用了阶跃函数（阶跃函数是指一旦输入超过阈值，就切换输出的函数。）的模型。“多层感知机”是指神经网络， 即使用 sigmoid 函数（后述)等平滑的激活函数的多层网络。

## 激活函数

​	式（3.3）表示的激活函数以阈值为界，一旦输入超过阈值，就切换输出。 这样的函数称为“阶跃函数”。因此，可以说感知机中使用了阶跃函数作为  激活函数。也就是说，在激活函数的众多候选函数中，感知机使用了阶跃函数。 那么，如果感知机使用其他函数作为激活函数的话会怎么样呢？实际上，如 果将激活函数从阶跃函数换成其他函数，就可以进入神经网络的世界了。下 面我们就来介绍一下神经网络使用的激活函数。

###  sigmoid 函数

神经网络中经常使用的一个激活函数就是式（3.6）表示的 sigmoid 函数 （sigmoid function）。

![image-20250820145522896](images/image-20250820145522896.png)

​	式（3.6）中的exp(-x) 表示 $e^{-x}$  的意思。e 是纳皮尔常数2.7182...。式（3.6） 表示的sigmoid 函数看上去有些复杂，但它也仅仅是个函数而已。而函数就是  给定某个输入后，会返回某个输出的转换器。比如，向sigmoid 函数输入1.0或2.0  后，就会有某个值被输出，类似h(1.0) = 0.731 ...、h(2.0) = 0.880... 这样。

​	神经网络中用sigmoid 函数作为激活函数，进行信号的转换，转换后的信号被传送给下一个神经元。实际上，上一章介绍的感知机和接下来要介绍 的神经网络的主要区别就在于这个激活函数。其他方面， 比如神经元的多层 连接的构造、信号的传递方法等，基本上和感知机是一样的。下面，让我们 通过和阶跃函数的比较来详细学习作为激活函数的sigmoid 函数。

### 阶跃函数的实现

​	这里我们试着用Python 画出阶跃函数的图（从视觉上确认函数的形状对  理解函数而言很重要）。阶跃函数如式（3.3）所示，当输入超过 0 时，输出1， 否则输出 0。可以像下面这样简单地实现阶跃函数。

```py
def  step_function(x):
	if  x  >  0:
		return  1
	else:
		return  0
```

​	这个实现简单、易于理解，但是参数 x 只能接受实数（浮点数）。也就是 说，允许形如step_function(3.0) 的调用，但不允许参数取NumPy 数组，例 如 **step_function(np.array([1.0, 2.0]))**。为了便于后面的操作，我们把它修 改为支持NumPy 数组的实现。为此，可以考虑下述实现。

```py
def step_function(x):
	y = x > 0
return y.astype(int)
```

​	这个实现简单、易于理解，但是参数 x 只能接受实数（浮点数）。也就是 说，允许形如step_function(3.0) 的调用，但不允许参数取NumPy 数组，例 如 step_function(np.array([1.0, 2.0]))。为了便于后面的操作，我们把它修 改为支持NumPy 数组的实现。为此，可以考虑下述实现。

```py
def  step_function(x):
	y  =  x  >  0
	return  y.astype(np.int)
```

​	上述函数的内容只有两行。由于使用了NumPy 中的“技巧”，可能会有  点难理解。下面我们通过Python 解释器的例子来看一下这里用了什么技巧。 下面这个例子中准备了NumPy 数组 x，并对这个NumPy 数组进行了不等号  运算。

```py
>>>  import  numpy  as  np
>>>  x  =  np.array([-1.0,  1.0,  2.0])
>>>  x
array([ -1 . ,     1 . ,    2 . ])
>>>  y  =  x  >  0
>>>  y
array([False,    True,    True],  dtype=bool)
```

​	对NumPy 数组进行不等号运算后，数组的各个元素都会进行不等号运算， 生成一个布尔型数组。这里，数组 x 中大于 0 的元素被转换为True，小于等 于 0 的元素被转换为False，从而生成一个新的数组y。

数组y 是一个布尔型数组，但是我们想要的阶跃函数是会输出int 型的 0 或 1 的函数。因此，需要把数组y 的元素类型从布尔型转换为int 型。

```py
>>>  y  =  y.astype(np.int)
>>>  y
array([0,  1,  1])
```

​	如上所示，可以用astype() 方法转换 NumPy 数组的类型。astype() 方 法通过参数指定期望的类型，这个例子中是 np.int 型。Python 中将布尔型 转换为int 型后，True 会转换为 1 ，False 会转换为 0。以上就是阶跃函数的 实现中所用到的NumPy 的“技巧”。

### 阶跃函数的图形

下面我们就用图来表示上面定义的阶跃函数，为此需要使用matplotlib 库。

```py
import numpy as np
import matplotlib.pylab as plt

def step_function(x):
 return np.array(x > 0, dtype=int)

x = np.arange(-5.0, 5.0, 0.1)
y = step_function(x)
plt.plot(x, y)
plt.ylim(-0.1, 1.1) # 指定y轴的范围
plt.show()
```

​	np.a range(-5.0,  5.0,  0.1) 在 -5.0 到 5.0 的范围内，以 0.1 为单位，生成 NumPy 数组（ [-5.0, -4.9,  . . . ,  4.9]）。step_function() 以该NumPy 数组为 参数，对数组的各个元素执行阶跃函数运算，并以数组形式返回运算结果。 对数组 x 、y 进行绘图，结果如图 3-6 所示。

![image-20250820153432437](images/image-20250820153432437.png)

如图 3-6 所示，阶跃函数以 0 为界，输出从 0 切换为 1（或者从 1 切换为 0）。 它的值呈阶梯式变化，所以称为阶跃函数。

### sigmoid 函数的实现

下面，我们来实现sigmoid 函数。用Python 可以像下面这样写出式（3.6） 表示的sigmoid 函数。

```py
def  sigmoid(x):
	return  1  /   (1  +  np.exp(-x))
```

​	这里， np.exp(-x) 对应exp(-x)。这个实现没有什么特别难的地方，但 是要注意参数 x 为NumPy 数组时，结果也能被正确计算。实际上，如果在 这个sigmoid 函数中输入一个NumPy 数组，则结果如下所示。

```py
>>>  x  =  np.array([-1.0,  1.0,  2.0])
>>>  sigmoid(x)
array([  0.26894142,    0.73105858,    0.88079708])
```

​	之所以sigmoid 函数的实现能支持NumPy 数组，秘密就在于NumPy 的 广播功能。根据NumPy 的广播功能，如果在标量和NumPy 数组 之间进行运算，则标量会和NumPy 数组的各个元素进行运算。这里来看一 个具体的例子。

```py
>>>  t  =  np.array([1.0,  2.0,  3.0])
>>>  1.0  +  t
array([  2 . ,    3 . ,    4 . ])
>>>  1.0  /  t
array([  1. , 0.5, 0.33333333])
```

​	在这个例子中，标量（例子中是1.0）和NumPy 数组之间进行了数值运 算（+ 、/ 等）。结果，标量和 NumPy 数组的各个元素进行了运算，运算结 果以 NumPy 数组的形式被输出。刚才的 sigmoid 函数的实现也是如此， 因 为np.exp(-x) 会生成NumPy 数组，所以1  /  (1  +  np.exp(-x)) 的运算将会在 NumPy 数组的各个元素间进行。

下面我们把sigmoid 函数画在图上。画图的代码和刚才的阶跃函数的代 码几乎是一样的，唯一不同的地方是把输出y 的函数换成了sigmoid 函数。

```py
import numpy as np
import matplotlib.pylab as plt
def  sigmoid(x):
    return  1  /   (1  +  np.exp(-x))
x  =  np.arange(-5.0,  5.0,  0.1)
y  =  sigmoid(x)
plt.plot(x,  y)
plt.ylim( -0.1, 1.1)  #  指定 y 轴的范围
plt.show()
```

![image-20250820154606582](images/image-20250820154606582.png)

### sigmoid 函数和阶跃函数的比较

​	现在我们来比较一下sigmoid 函数和阶跃函数，如图 3-8 所示。两者的 不同点在哪里呢？又有哪些共同点呢？我们通过观察图 3-8 来思考一下。

​	观察图 3-8，首先注意到的是“平滑性”的不同。sigmoid 函数是一条平 滑的曲线，输出随着输入发生连续性的变化。而阶跃函数以 0 为界，输出发 生急剧性的变化。sigmoid 函数的平滑性对神经网络的学习具有重要意义。

![image-20250820154920631](images/image-20250820154920631.png)

​	另一个不同点是，相对于阶跃函数只能返回 0 或1 ，sigmoid 函数可以返 回 0.731 ... 、0.880 ... 等实数（这一点和刚才的平滑性有关）。也就是说，感 知机中神经元之间流动的是 0 或 1 的二元信号，而神经网络中流动的是连续 的实数值信号。

​	接着说一下阶跃函数和 sigmoid 函数的共同性质。阶跃函数和 sigmoid  函数虽然在平滑性上有差异，但是如果从宏观视角看图 3-8，可以发现它们 具有相似的形状。实际上，两者的结构均是“输入小时，输出接近 0（为 0）； 随着输入增大，输出向 1 靠近（变成 1）”。也就是说，当输入信号为重要信息时， 阶跃函数和sigmoid 函数都会输出较大的值；当输入信号为不重要的信息时， 两者都输出较小的值。还有一个共同点是，不管输入信号有多小，或者有多  大，输出信号的值都在 0 到 1 之间。

### 非线性函数

​	阶跃函数和sigmoid 函数还有其他共同点，就是两者均为非线性函数。 sigmoid 函数是一条曲线，阶跃函数是一条像阶梯一样的折线，两者都属于 非线性的函数。

​	在介绍激活函数时，经常会看到“非线性函数”和“线性函数”等术语。 函数本来是输入某个值后会返回一个值的转换器。向这个转换器输  入某个值后，输出值是输入值的常数倍的函数称为线性函数（用数学  式表示为 h(x) = cx。c 为常数）。因此，线性函数是一条笔直的直线。 而非线性函数，顾名思义，指的是不像线性函数那样呈现出一条直  线的函数。

​	神经网络的激活函数必须使用非线性函数。换句话说，激活函数不能使 用线性函数。为什么不能使用线性函数呢？ 因为使用线性函数的话，加深神 经网络的层数就没有意义了。

​	线性函数的问题在于，不管如何加深层数，总是存在与之等效的“无 隐藏层的神经网络”。为了具体地（稍微直观地）理解这一点，我们来思 考下面这个简单的例子。这里我们考虑把线性函数 h(x) = cx 作为激活 函数，把 y(x) = h(h(h(x))) 的运算对应 3 层神经网络（该对应只是一个近似，实际的神经网络运算比这个例子要复杂，但不影响后面的结论成立。）。这个运算会进行 y(x) = c × c × c × x 的乘法运算，但是同样的处理可以由 y(x) = ax（注意， a = c 3）这一次乘法运算（即没有隐藏层的神经网络）来表示。如本例所示， 使用线性函数时，无法发挥多层网络带来的优势。因此，为了发挥叠加层所  带来的优势，激活函数必须使用非线性函数。

### ReLU函数

​	到目前为止，我们介绍了作为激活函数的阶跃函数和sigmoid 函数。在 神经网络发展的历史上，sigmoid 函数很早就开始被使用了，而最近则主要 使用 **ReLU**（Rectified Linear Unit）函数。

ReLU 函数在输入大于 0 时，直接输出该值；在输入小于等于 0 时，输 出 0（图 3-9）。

ReLU 函数可以表示为下面的式(3.7)。

![image-20250820155759122](images/image-20250820155759122.png)

如图 3-9 和式（3.7）所示，ReLU 函数是一个非常简单的函数。因此， ReLU 函数的实现也很简单，可以写成如下形式。

```py
def  relu(x):
return  np .maximum(0,  x)
```

![image-20250820155830243](images/image-20250820155830243.png)

这里使用了NumPy 的maximum 函数。maximum 函数会从输入的数值中选 择较大的那个值进行输出。

本章剩余部分的内容仍将使用sigmoid 函数作为激活函数，但在本书的 后半部分，则将主要使用ReLU 函数。

## 多维数组的运算

如果掌握了NumPy 多维数组的运算，就可以高效地实现神经网络。因此， 本节将介绍NumPy 多维数组的运算，然后再进行神经网络的实现。

### 多维数组

​	简单地讲，多维数组就是“数字的集合”，数字排成一列的集合、排成 长方形的集合、排成三维状或者（更加一般化的）N维状的集合都称为多维数 组。下面我们就用NumPy 来生成多维数组，先从前面介绍过的一维数组开始。

```py
>>>  import  numpy  as  np
>>>  A  =  np.array([1,  2,  3,  4])
>>>  print(A)
[1  2  3  4]
>>>  np.ndim(A)
1
>>>  A .shape
(4,)
>>>  A.shape[0]
4
```

​	如上所示，数组的维数可以通过np.dim()函数获得。此外，数组的形状 可以通过实例变量shape 获得。在上面的例子中，A 是一维数组， 由 4 个元素 构成。注意，这里的 A.shape 的结果是个元组（tuple）。这是因为一维数组的 情况下也要返回和多维数组的情况下一致的结果。例如，二维数组时返回的 是元组(4,3)，三维数组时返回的是元组(4,3,2)，因此一维数组时也同样以 元组的形式返回结果。下面我们来生成一个二维数组。

```py
>>>  B  =  np.array([[1,2],  [3,4],   [5,6]])
>>>  print(B)
[[1  2]
[3  4]
[5  6]]
>>>  np.ndim(B)
2
>>>  B .shape
(3,  2)
```

​	这里生成了一个 3 × 2 的数组 B 。3 × 2 的数组表示第一个维度有 3 个元素， 第二个维度有 2 个元素。另外，第一个维度对应第 0 维，第二个维度对应第 1 维（Python 的索引从 0 开始）。二维数组也称为矩阵（matrix）。如图 3-10 所示， 数组的横向排列称为行（row），纵向排列称为列（column）。

### 矩阵乘法

下面，我们来介绍矩阵（二维数组）的乘积。比如 2 × 2 的矩阵，其乘积 可以像图 3-11 这样进行计算（按图中顺序进行计算是规定好了的）。

![image-20250820161236362](images/image-20250820161236362.png)

​	如本例所示，矩阵的乘积是通过左边矩阵的行（横向）和右边矩阵的列（纵 向）以对应元素的方式相乘后再求和而得到的。并且，运算的结果保存为新 的多维数组的元素。比如，***\*A\****的第 1 行和***\*B\****的第 1 列的乘积结果是新数组的 第 1 行第 1 列的元素，***\*A\****的第 2 行和***\*B\****的第 1 列的结果是新数组的第 2 行第1 列的元素。另外，在本书的数学标记中，矩阵将用黑斜体表示（比如，矩阵 ***\*A\****），以区别于单个元素的标量（比如，a 或 b）。这个运算在Python 中可以用 如下代码实现。

```py
>>>  A  =  np.array([[1,2],   [3,4]])
>>>  A .shape
(2,  2)
>>>  B  =  np.array([[5,6],   [7,8]])
>>>  B .shape
(2,  2)
>>>  np.dot(A,  B)
array([[19,  22],
[43,  50]])
```

​	这 里，**A** 和 **B** 都是2 × 2 的矩阵，它们的乘积可以通过 NumPy 的 np.dot() 函数计算（乘积也称为点积）。np.dot() 接收两个NumPy 数组作为参 数，并返回数组的乘积。这里要注意的是，np.dot(A,  B) 和np.dot(B,  A) 的 值可能不一样。和一般的运算（+ 或* 等）不同，矩阵的乘积运算中，操作数（A、 B）的顺序不同，结果也会不同。

​	这里介绍的是计算 2 × 2 形状的矩阵的乘积的例子，其他形状的矩阵的 乘积也可以用相同的方法来计算。比如，2 × 3 的矩阵和 3 × 2 的矩阵的乘积 可按如下形式用Python 来实现。

```py
>>>  A  =  np.array([[1,2,3],  [4,5,6]])
>>>  A .shape
(2,  3)
>>>  B  =  np.array([[1,2],  [3,4],   [5,6]])
>>>  B .shape
(3,  2)
>>>  np.dot(A,  B)
array([[22,  28],
[49,  64]])
```

​	2 × 3 的矩阵A和 3 × 2 的矩阵B的乘积可按以上方式实现。这里需要   注意的是矩阵的形状（shape）。具体地讲，矩阵A的第 1 维的元素个数（列数） 必须和矩阵B的第 0 维的元素个数（行数）相等。在上面的例子中，矩阵A   的形状是 2 × 3，矩阵B的形状是 3 × 2，矩阵A的第 1 维的元素个数（3）和   矩阵B的第 0 维的元素个数（3）相等。如果这两个值不相等，则无法计算矩   阵的乘积。比如，如果用Python 计算 2 × 3 的矩阵A和 2 × 2 的矩阵C的乘   积，则会输出如下错误。

```py
>>>  C  =  np.array([[1,2],   [3,4]])
>>>  C .shape
(2,  2)
>>>  A .shape
(2,  3)
>>>  np.dot(A,  C)
Traceback  (most  recent  call  last):
File  "<stdin>",  line  1,  in  <module>
ValueError:  shapes  (2,3)  and   (2,2)  not  aligned:  3   (dim  1)   !=  2  (dim  0)
```

​	这个错误的意思是，矩阵 **A** 的第 1 维和矩阵 **C** 的第 0 维的元素个数不一 致（维度的索引从 0 开始）。也就是说，在多维数组的乘积运算中，必须使两 个矩阵中的对应维度的元素个数一致，这一点很重要。我们通过图 3-12 再来 确认一下。

![image-20250820162242832](images/image-20250820162242832.png)

​	图 3-12 中，3 × 2 的矩阵 **A **和 2 × 4 的矩阵 **B** 的乘积运算生成了3 × 4 的  矩阵C。如图所示，矩阵 **A** 和矩阵 **B** 的对应维度的元素个数必须保持一致。 此外，还有一点很重要，就是运算结果的矩阵C的形状是由矩阵 **A** 的行数  和矩阵 **B** 的列数构成的。
另外，当 **A** 是二维矩阵、**B** 是一维数组时，如图 3-13 所示，对应维度 的元素个数要保持一致的原则依然成立。
可按如下方式用Python 实现图 3-13 的例子。

```py
>>>  A  =  np.array([[1,2],  [3,  4],   [5,6]])
>>>  A .shape
(3,  2)
>>>  B  =  np.array([7,8])
>>>  B .shape
(2,)
>>>  np.dot(A,  B)
array([23,  53,  83])
```

![image-20250820162532293](images/image-20250820162532293.png)

### 神经网络的内积

下面我们使用NumPy 矩阵来实现神经网络。这里我们以图 3-14 中的简 单神经网络为对象。这个神经网络省略了偏置和激活函数，只有权重。

![image-20250820164045961](images/image-20250820164045961.png)

实现该神经网络时，要注意X 、W、Y的形状，特别是X和W的对应 维度的元素个数是否一致，这一点很重要。

```py
>>>  X  =  np.array([1,  2])
>>>  X .shape
(2,)
>>>  W  =  np.array([[1,  3,  5],   [2,  4,  6]])
>>>  print(W)
[[1  3  5]
[2  4  6]]
>>>  W .shape
(2,  3)
>>>  Y  =  np.dot(X,  W)
>>>  print(Y)
[  5     11     17]
```

​	如上所示，使用 np.dot（多维数组的点积），可以一次性计算出***\*Y\**** 的结果。 这意味着，即便***\*Y\****的元素个数为100 或1000，也可以通过一次运算就计算出 结果！如果不使用 np.dot，就必须单独计算***\*Y\****的每一个元素（或者说必须使 用for 语句），非常麻烦。因此，通过矩阵的乘积一次性完成计算的技巧，在 实现的层面上可以说是非常重要的。

## 3层神经网络的实现

​	现在我们来进行神经网络的实现。这里我们以图 3-15 的 3 层神经网络为 对象，实现从输入到输出的（前向）处理。在代码实现方面，使用上一节介 绍的 NumPy 多维数组。巧妙地使用NumPy 数组，可以用很少的代码完成 神经网络的前向处理。

![image-20250820164534599](images/image-20250820164534599.png)

### 符号确认

​	在介绍神经网络中的处理之前，我们先导入 $w_{12}^{(1)}$ 、$a_1^{(1)}$" 等符号。这些符 号可能看上去有些复杂，不过因为只在本节使用，稍微读一下就跳过去也问 题不大。

​	本节的重点是神经网络的运算可以作为矩阵运算打包进行。因为 神经网络各层的运算是通过矩阵的乘法运算打包进行的（从宏观 视角来考虑），所以即便忘了（未记忆）具体的符号规则，也不影 响理解后面的内容。

我们先从定义符号开始。请看图3-16。图 3-16 中只突出显示了从输入层 神经元 x2 到后一层的神经元 af) 的权重。

​	如图 3-16 所示，权重和隐藏层的神经元的右上角有一个 “(1)”，它表示 权重和神经元的层号（即第 1 层的权重、第 1 层的神经元）。此外，权重的右 下角有两个数字，它们是后一层的神经元和前一层的神经元的索引号。比如， $w_{12}^{(1)}$ 表示前一层的第 2 个神经元 x2 到后一层的第 1 个神经元$a_1^{(1)}$ 的权重。权 重右下角按照“后一层的索引号、前一层的索引号”的顺序排列。

![image-20250820172123247](images/image-20250820172123247.png)
